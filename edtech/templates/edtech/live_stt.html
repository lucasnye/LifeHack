<!-- <!DOCTYPE html>
<html>
    <head>
        <title>Live Speech-to-Text</title>
        <style>
            body { font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }
            #transcript { min-height: 200px; border: 1px solid #ddd; padding: 15px; margin: 20px 0; }
            .btn { padding: 10px 20px; font-size: 16px; cursor: pointer; }
            .recording { background-color: #ff4d4d; color: white; }
        </style>
    </head>
    <body>
        <h1>Live Speech-to-Text</h1>
        
        <button id="startBtn" class="btn">Start Recording</button>
        <button id="stopBtn" class="btn" disabled>Stop Recording</button>
        
        <div id="transcript"></div>
        
        <script>
            class AudioRecorder {
                constructor() {
                    this.audioContext = null;
                    this.mediaStream = null;
                    this.audioProcessor = null;
                    this.isRecording = false;
                    this.socket = null;
                    this.bufferSize = 4096;
                    this.sampleRate = 16000;
                }

                async start() {
                    try {
                        // Initialize WebSocket
                        this.socket = new WebSocket(`ws://${window.location.host}/ws/live_stt/`);
                        
                        // Get microphone access
                        this.mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                        
                        // Create audio context
                        this.audioContext = new (window.AudioContext || window.webkitAudioContext)({
                            sampleRate: this.sampleRate
                        });
                        
                        // Create audio source
                        const source = this.audioContext.createMediaStreamSource(this.mediaStream);
                        
                        // Create processor node
                        this.audioProcessor = this.audioContext.createScriptProcessor(
                            this.bufferSize, 
                            1, 
                            1
                        );
                        
                        // Configure processor
                        this.audioProcessor.onaudioprocess = (event) => {
                            if (!this.isRecording || !this.socket || this.socket.readyState !== WebSocket.OPEN) 
                                return;
                            
                            const inputData = event.inputBuffer.getChannelData(0);
                            const int16Data = this.convertFloat32ToInt16(inputData);
                            this.socket.send(int16Data);
                        };
                        
                        // Connect nodes
                        source.connect(this.audioProcessor);
                        this.audioProcessor.connect(this.audioContext.destination);
                        
                        this.isRecording = true;
                        return true;
                    } catch (error) {
                        console.error('Recording error:', error);
                        return false;
                    }
                }

                stop() {
                    if (this.audioProcessor) {
                        this.audioProcessor.disconnect();
                        this.audioProcessor = null;
                    }
                    
                    if (this.mediaStream) {
                        this.mediaStream.getTracks().forEach(track => track.stop());
                        this.mediaStream = null;
                    }
                    
                    if (this.audioContext) {
                        this.audioContext.close();
                        this.audioContext = null;
                    }
                    
                    if (this.socket) {
                        this.socket.close();
                        this.socket = null;
                    }
                    
                    this.isRecording = false;
                }

                convertFloat32ToInt16(buffer) {
                    const length = buffer.length;
                    const int16Array = new Int16Array(length);
                    
                    for (let i = 0; i < length; i++) {
                        const s = Math.max(-1, Math.min(1, buffer[i]));
                        int16Array[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
                    }
                    
                    return int16Array.buffer;
                }
            }

            // UI Controller
            document.addEventListener('DOMContentLoaded', () => {
                const recorder = new AudioRecorder();
                const startBtn = document.getElementById('startBtn');
                const stopBtn = document.getElementById('stopBtn');
                const transcriptDiv = document.getElementById('transcript');
                
                startBtn.addEventListener('click', async () => {
                    const success = await recorder.start();
                    if (success) {
                        startBtn.disabled = true;
                        stopBtn.disabled = false;
                        startBtn.classList.add('recording');
                        transcriptDiv.textContent = "Listening...";
                    }
                });
                
                stopBtn.addEventListener('click', () => {
                    recorder.stop();
                    startBtn.disabled = false;
                    stopBtn.disabled = true;
                    startBtn.classList.remove('recording');
                    transcriptDiv.textContent += "\n\nStopped recording";
                });
                
                // Handle WebSocket messages
                if (recorder.socket) {
                    recorder.socket.onmessage = (event) => {
                        const data = JSON.parse(event.data);
                        if (data.transcription) {
                            transcriptDiv.textContent = data.transcription;
                        } else if (data.error) {
                            transcriptDiv.textContent = `Error: ${data.error}`;
                        }
                    };
                    
                    recorder.socket.onerror = (error) => {
                        transcriptDiv.textContent = `WebSocket error: ${error.message}`;
                    };
                }
            });
        </script>
    </body>
</html> -->


<!DOCTYPE html>
<html>
<head>
    <title>Live STT Debug</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        #debug {
            font-family: monospace;
            white-space: pre;
            background: #111;
            color: #0f0;
            padding: 10px;
            height: 200px;
            overflow-y: auto;
        }
        #transcript {
            background: #f5f5f5;
            padding: 15px;
            margin-top: 20px;
            min-height: 100px;
            border: 1px solid #ddd;
            white-space: pre-wrap;
        }
        .error { color: red; }
        .btn {
            padding: 10px 20px;
            font-size: 16px;
            cursor: pointer;
            margin-right: 10px;
        }
        .recording { background-color: #ff4d4d; color: white; }
        .stopped { background-color: #4CAF50; color: white; }
    </style>
</head>
<body>
    <h1>Live Speech-to-Text</h1>
    <button id="startBtn" class="btn">Start Recording</button>
    <button id="stopBtn" class="btn" disabled>Stop Recording</button>
    <div id="transcript">Ready for transcription...</div>
    <div id="debug"></div>

    <script>
        const debugEl = document.getElementById('debug');
        const transcriptEl = document.getElementById('transcript');
        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        
        let ws;
        let audioContext;
        let mediaStream;
        let audioProcessor;

        const log = (msg, isError = false) => {
            const timestamp = new Date().toLocaleTimeString();
            debugEl.innerHTML += `<span class="${isError ? 'error' : ''}">[${timestamp}] ${msg}\n</span>`;
            debugEl.scrollTop = debugEl.scrollHeight;
        };

        const stopRecording = () => {
            if (audioProcessor) {
                audioProcessor.disconnect();
                audioProcessor = null;
            }
            
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                mediaStream = null;
            }
            
            if (audioContext) {
                audioContext.close();
                audioContext = null;
            }
            
            if (ws) {
                ws.close();
                ws = null;
            }
            
            startBtn.disabled = false;
            stopBtn.disabled = true;
            startBtn.classList.remove('recording');
            stopBtn.classList.add('stopped');
            log("⏹️ Recording stopped");
            transcriptEl.textContent += "\n\n[Recording stopped]";
        };

        startBtn.onclick = async () => {
            try {
                startBtn.disabled = true;
                stopBtn.disabled = false;
                startBtn.classList.add('recording');
                stopBtn.classList.remove('stopped');
                transcriptEl.textContent = "Initializing...";
                
                // 1. Get microphone access
                mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                log("🎤 Microphone access granted");
                
                // 2. Setup WebSocket
                ws = new WebSocket(`ws://${window.location.host}/ws/live_stt/`);
                
                ws.onmessage = (event) => {
                    try {
                        const data = JSON.parse(event.data);
                        log(`Received: ${JSON.stringify(data)}`);
                        
                        if (data.transcription) {
                            transcriptEl.textContent += data.transcription + " ";
                            transcriptEl.scrollTop = transcriptEl.scrollHeight;
                        }
                    } catch (e) {
                        log(`Message parse error: ${e}`, true);
                    }
                };
                
                ws.onopen = () => {
                    log("🟢 WebSocket connected");
                    transcriptEl.textContent = "Listening...";
                };
                
                ws.onerror = (e) => log(`WebSocket ERROR: ${e.message}`, true);
                ws.onclose = () => log("WebSocket disconnected");
                
                // 3. Audio processing
                audioContext = new (window.AudioContext || window.webkitAudioContext)({
                    sampleRate: 16000
                });
                
                audioProcessor = audioContext.createScriptProcessor(4096, 1, 1);
                let chunkCount = 0;
                
                audioProcessor.onaudioprocess = (e) => {
                    const audioData = e.inputBuffer.getChannelData(0);
                    const rms = Math.sqrt(audioData.reduce((sum, x) => sum + x*x, 0) / audioData.length);
                    
                    const volumeBar = '■'.repeat(Math.floor(rms * 20));
                    log(`Chunk ${++chunkCount} ${volumeBar} RMS: ${rms.toFixed(4)}`);
                    
                    if (ws && ws.readyState === WebSocket.OPEN) {
                        ws.send(new Int16Array(audioData.map(x => x * 32767)).buffer);
                    }
                };
                
                audioContext.createMediaStreamSource(mediaStream)
                    .connect(audioProcessor);
                audioProcessor.connect(audioContext.destination);
                
                log("⏺️ Recording started");
                
            } catch (err) {
                log(`❌ CRASH: ${err}`, true);
                transcriptEl.textContent = "Error: " + err.message;
                stopRecording();
            }
        };

        stopBtn.onclick = stopRecording;

        window.addEventListener('beforeunload', stopRecording);
    </script>
</body>
</html>